#!/usr/bin/env python3
"""
Workflow Runner v2.0 - å·¥ä½œæµæ‰§è¡Œå™¨
æ”¯æŒå…¨å±€å·¥ä½œæµå’Œé¡¹ç›®å·¥ä½œæµä¸¤ç§æ¨¡å¼
ğŸ†• æ”¯æŒèœ‚ç¾¤æ¨¡å¼å¹¶è¡Œæ‰§è¡Œ
"""

import json
import subprocess
import sys
import os
import time
import threading
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict
from enum import Enum

GLOBAL_WORKFLOW_DIR = Path("C:/Users/Administrator/.trae-cn/workflows")
PROJECT_WORKFLOW_DIR = Path(".trae/workflows")
SWARM_DIR = Path(".trae/swarm")


class TaskStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class SwarmTask:
    task_id: str
    description: str
    worker_type: str
    dependencies: List[str]
    status: str = "pending"
    result: Optional[dict] = None


class SwarmOrchestrator:
    """èœ‚ç¾¤è°ƒåº¦å™¨ - ç®¡ç†å¹¶è¡Œä»»åŠ¡æ‰§è¡Œ"""
    
    def __init__(self, project_path: str = "."):
        self.project_path = Path(project_path)
        self.swarm_path = self.project_path / SWARM_DIR
        self.queue_path = self.swarm_path / "queue.json"
        self.results_path = self.swarm_path / "results"
        
        self._ensure_directories()
    
    def _ensure_directories(self):
        self.swarm_path.mkdir(parents=True, exist_ok=True)
        self.results_path.mkdir(parents=True, exist_ok=True)
    
    def analyze_complexity(self, task_description: str) -> dict:
        """åˆ†æä»»åŠ¡å¤æ‚åº¦"""
        complex_keywords = ["ç³»ç»Ÿ", "æ¶æ„", "é‡æ„", "å¼€å‘", "å®ç°", "æ„å»º", "è®¾è®¡"]
        simple_keywords = ["ä¿®æ”¹", "ä¿®å¤", "ä¼˜åŒ–", "æ·»åŠ ", "æ›´æ–°"]
        
        complexity_score = sum(2 for kw in complex_keywords if kw in task_description)
        complexity_score += sum(1 for kw in simple_keywords if kw in task_description)
        
        if complexity_score >= 4:
            return {"complexity": "complex", "swarm_mode": True}
        elif complexity_score >= 2:
            return {"complexity": "medium", "swarm_mode": False}
        else:
            return {"complexity": "simple", "swarm_mode": False}
    
    def decompose_task(self, task_description: str) -> List[SwarmTask]:
        """å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡"""
        tasks = [
            SwarmTask(
                task_id="task_001",
                description="è°ƒç ”æŠ€æœ¯æ–¹æ¡ˆ",
                worker_type="researcher",
                dependencies=[]
            ),
            SwarmTask(
                task_id="task_002",
                description="è®¾è®¡ç³»ç»Ÿæ¶æ„",
                worker_type="coder",
                dependencies=["task_001"]
            ),
            SwarmTask(
                task_id="task_003",
                description="å®ç°æ ¸å¿ƒåŠŸèƒ½",
                worker_type="coder",
                dependencies=["task_002"]
            ),
            SwarmTask(
                task_id="task_004",
                description="ç¼–å†™æµ‹è¯•ç”¨ä¾‹",
                worker_type="tester",
                dependencies=["task_003"]
            ),
            SwarmTask(
                task_id="task_005",
                description="ç¼–å†™æ–‡æ¡£",
                worker_type="writer",
                dependencies=["task_003"]
            ),
            SwarmTask(
                task_id="task_006",
                description="ä»£ç å®¡æŸ¥",
                worker_type="reviewer",
                dependencies=["task_003", "task_004", "task_005"]
            )
        ]
        return tasks
    
    def build_execution_order(self, tasks: List[SwarmTask]) -> List[List[SwarmTask]]:
        """æ„å»ºæ‰§è¡Œå±‚çº§ï¼ˆDAGè°ƒåº¦ï¼‰"""
        task_map = {t.task_id: t for t in tasks}
        completed = set()
        levels = []
        remaining = set(t.task_id for t in tasks)
        
        while remaining:
            ready = []
            for tid in remaining:
                task = task_map[tid]
                if all(d in completed for d in task.dependencies):
                    ready.append(task)
            
            if not ready:
                break
            
            levels.append(ready)
            for t in ready:
                completed.add(t.task_id)
                remaining.discard(t.task_id)
        
        return levels
    
    def execute_parallel(self, tasks: List[SwarmTask], max_workers: int = 3) -> dict:
        """å¹¶è¡Œæ‰§è¡Œä»»åŠ¡"""
        levels = self.build_execution_order(tasks)
        all_results = {}
        
        for level_idx, level_tasks in enumerate(levels):
            print(f"\n=== æ‰§è¡Œå±‚çº§ {level_idx + 1}/{len(levels)} ===")
            print(f"å¹¶è¡Œä»»åŠ¡æ•°: {len(level_tasks)}")
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = {}
                for task in level_tasks:
                    future = executor.submit(self._execute_single_task, task)
                    futures[future] = task
                
                for future in as_completed(futures):
                    task = futures[future]
                    try:
                        result = future.result()
                        all_results[task.task_id] = result
                        task.status = "completed"
                        print(f"âœ… {task.task_id}: {task.description}")
                    except Exception as e:
                        task.status = "failed"
                        all_results[task.task_id] = {"error": str(e)}
                        print(f"âŒ {task.task_id}: {str(e)}")
        
        return {
            "status": "completed",
            "total_tasks": len(tasks),
            "completed": sum(1 for t in tasks if t.status == "completed"),
            "failed": sum(1 for t in tasks if t.status == "failed"),
            "results": all_results
        }
    
    def _execute_single_task(self, task: SwarmTask) -> dict:
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡"""
        task.status = "running"
        time.sleep(0.5)  # æ¨¡æ‹Ÿæ‰§è¡Œ
        return {
            "task_id": task.task_id,
            "worker": task.worker_type,
            "status": "completed",
            "output": f"å®Œæˆ: {task.description}"
        }


def get_workflow_dirs() -> list[Path]:
    """è·å–æ‰€æœ‰å·¥ä½œæµç›®å½•ï¼ˆé¡¹ç›®çº§ä¼˜å…ˆï¼‰"""
    dirs = []
    
    project_dir = Path.cwd() / PROJECT_WORKFLOW_DIR
    if project_dir.exists():
        dirs.append(project_dir)
    
    if GLOBAL_WORKFLOW_DIR.exists():
        dirs.append(GLOBAL_WORKFLOW_DIR)
    
    return dirs


def find_workflow_manager() -> Path | None:
    """æŸ¥æ‰¾å¯ç”¨çš„ workflow_manager.py"""
    for dir_path in get_workflow_dirs():
        manager = dir_path / "workflow_manager.py"
        if manager.exists():
            return manager
    return None


def list_workflows() -> list:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨å·¥ä½œæµï¼ˆåˆå¹¶å…¨å±€å’Œé¡¹ç›®ï¼‰"""
    all_workflows = []
    seen_names = set()
    
    for dir_path in get_workflow_dirs():
        try:
            result = subprocess.run(
                [sys.executable, str(dir_path / "workflow_manager.py"), "list"],
                capture_output=True,
                text=True,
                timeout=10
            )
            if result.returncode == 0:
                workflows = json.loads(result.stdout)
                for wf in workflows:
                    if isinstance(wf, dict) and "name" in wf:
                        if wf["name"] not in seen_names:
                            seen_names.add(wf["name"])
                            wf["source"] = "project" if dir_path == Path.cwd() / PROJECT_WORKFLOW_DIR else "global"
                            all_workflows.append(wf)
                    elif isinstance(wf, dict) and "error" not in wf:
                        all_workflows.append(wf)
        except Exception as e:
            all_workflows.append({"error": f"{dir_path}: {str(e)}"})
    
    return all_workflows


def find_workflow_location(workflow_name: str) -> Path | None:
    """æŸ¥æ‰¾å·¥ä½œæµæ‰€åœ¨çš„ç›®å½•ï¼ˆé¡¹ç›®çº§ä¼˜å…ˆï¼‰"""
    for dir_path in get_workflow_dirs():
        workflow_file = dir_path / f"{workflow_name}.yaml"
        if workflow_file.exists():
            return dir_path
    return None


def run_workflow(workflow_name: str, context: dict = None) -> dict:
    """æ‰§è¡ŒæŒ‡å®šå·¥ä½œæµ"""
    workflow_dir = find_workflow_location(workflow_name)
    
    if not workflow_dir:
        return {"status": "error", "message": f"å·¥ä½œæµ '{workflow_name}' æœªæ‰¾åˆ°"}
    
    try:
        cmd = [sys.executable, str(workflow_dir / "workflow_manager.py"), "run", workflow_name]
        
        if context:
            for key, value in context.items():
                cmd.extend(["--var", f"{key}={value}"])
        
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=60
        )
        
        if result.returncode == 0:
            return json.loads(result.stdout)
        else:
            return {"status": "error", "message": result.stderr}
    except Exception as e:
        return {"status": "error", "message": str(e)}


def run_swarm_workflow(task_description: str, max_workers: int = 3) -> dict:
    """ğŸ†• è¿è¡Œèœ‚ç¾¤å·¥ä½œæµ"""
    print(f"\nğŸ å¯åŠ¨èœ‚ç¾¤æ¨¡å¼...")
    print(f"ä»»åŠ¡æè¿°: {task_description}")
    
    orchestrator = SwarmOrchestrator()
    
    print(f"\nğŸ“Š åˆ†æä»»åŠ¡å¤æ‚åº¦...")
    analysis = orchestrator.analyze_complexity(task_description)
    print(f"å¤æ‚åº¦: {analysis['complexity']}")
    print(f"èœ‚ç¾¤æ¨¡å¼: {analysis['swarm_mode']}")
    
    if not analysis['swarm_mode']:
        print(f"\nâ­ï¸ ä»»åŠ¡ç®€å•ï¼Œè·³è¿‡èœ‚ç¾¤æ¨¡å¼")
        return {"status": "simple", "message": "ä»»åŠ¡ç®€å•ï¼Œå»ºè®®å•Agentæ‰§è¡Œ"}
    
    print(f"\nğŸ“‹ åˆ†è§£ä»»åŠ¡...")
    tasks = orchestrator.decompose_task(task_description)
    print(f"åˆ†è§£ä¸º {len(tasks)} ä¸ªå­ä»»åŠ¡:")
    for i, task in enumerate(tasks, 1):
        deps = f" (ä¾èµ–: {', '.join(task.dependencies)})" if task.dependencies else ""
        print(f"  {i}. [{task.worker_type}] {task.description}{deps}")
    
    print(f"\nğŸš€ å¼€å§‹å¹¶è¡Œæ‰§è¡Œ...")
    result = orchestrator.execute_parallel(tasks, max_workers)
    
    print(f"\nğŸ“Š æ‰§è¡Œå®Œæˆ!")
    print(f"æ€»ä»»åŠ¡: {result['total_tasks']}")
    print(f"å®Œæˆ: {result['completed']}")
    print(f"å¤±è´¥: {result['failed']}")
    
    return result


def find_workflow_by_trigger(text: str) -> str | None:
    """æ ¹æ®ç”¨æˆ·è¾“å…¥æŸ¥æ‰¾åŒ¹é…çš„å·¥ä½œæµ"""
    triggers = {
        "git-commit-summary": ["æäº¤æ‘˜è¦", "git summary", "å‘¨æŠ¥", "commit", "æäº¤è®°å½•"],
        "project-stats": ["ç»Ÿè®¡é¡¹ç›®", "project stats", "ä»£ç ç»Ÿè®¡", "ç»Ÿè®¡", "ä»£ç é‡"],
        "swarm-execution": ["èœ‚ç¾¤", "å¹¶è¡Œæ‰§è¡Œ", "swarm", "/swarm", "å¯åŠ¨èœ‚ç¾¤"]
    }
    
    text_lower = text.lower()
    for workflow, keywords in triggers.items():
        for keyword in keywords:
            if keyword.lower() in text_lower:
                if find_workflow_location(workflow):
                    return workflow
    return None


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Workflow Runner v2.0")
    parser.add_argument("action", choices=["list", "run", "detect", "swarm"])
    parser.add_argument("--workflow", help="å·¥ä½œæµåç§°")
    parser.add_argument("--text", help="ç”¨æˆ·è¾“å…¥æ–‡æœ¬ï¼ˆç”¨äºæ£€æµ‹ï¼‰")
    parser.add_argument("--task", help="ä»»åŠ¡æè¿°ï¼ˆç”¨äºèœ‚ç¾¤æ¨¡å¼ï¼‰")
    parser.add_argument("--workers", type=int, default=3, help="æœ€å¤§å¹¶è¡ŒWorkeræ•°")
    
    args = parser.parse_args()
    
    if args.action == "list":
        workflows = list_workflows()
        print(json.dumps(workflows, ensure_ascii=False, indent=2))
    elif args.action == "run":
        if not args.workflow:
            print(json.dumps({"error": "è¯·æŒ‡å®šå·¥ä½œæµåç§°"}, ensure_ascii=False))
        else:
            result = run_workflow(args.workflow)
            print(json.dumps(result, ensure_ascii=False, indent=2))
    elif args.action == "detect":
        if not args.text:
            print(json.dumps({"error": "è¯·æä¾›æ–‡æœ¬"}, ensure_ascii=False))
        else:
            workflow = find_workflow_by_trigger(args.text)
            print(json.dumps({
                "detected": workflow is not None,
                "workflow": workflow,
                "text": args.text
            }, ensure_ascii=False))
    elif args.action == "swarm":
        if not args.task:
            print(json.dumps({"error": "è¯·æä¾›ä»»åŠ¡æè¿°"}, ensure_ascii=False))
        else:
            result = run_swarm_workflow(args.task, args.workers)
            print(json.dumps(result, ensure_ascii=False, indent=2))
